{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iGED : Global Systems Dynamics Initiative\n",
    "\n",
    "El objetivo principal de este documento es analizar las métricas globales en las siguientes redes capitales\n",
    "- Ciudad Autónoma de Buenos Aires (CABA)\n",
    "- Ciudad de México (CDMX)\n",
    "- Santiago de Chile (SCL)\n",
    "- Montevideo (MTV)\n",
    "- Madrid (MAD)\n",
    "- Sao Paulo (SAO)\n",
    "\n",
    "y obtener un tidy DataFrame, para continuar analizando los datos obtenidos, por ejemplo obteniendo correlaciones entre parejas de métricas.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File Gephi_stats/Gephi AGS Stats.csv does not exist: 'Gephi_stats/Gephi AGS Stats.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5c5b36745c55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Archivo CSV describiendo los nodos de cada red\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#-------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mags_nd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Gephi_stats/Gephi AGS Stats.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mcaba_nd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Gephi_stats/Gephi CABA Stats.csv'\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m#capital\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mcdmx_nd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Gephi_stats/Gephi CDMX Stats.csv'\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m#capital\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File Gephi_stats/Gephi AGS Stats.csv does not exist: 'Gephi_stats/Gephi AGS Stats.csv'"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------\n",
    "# Importar paquetes a utilizar\n",
    "#-------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import networkx as nx\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "#-------------------------------------------------------\n",
    "# Archivo CSV describiendo los nodos de cada red\n",
    "#-------------------------------------------------------\n",
    "ags_nd=pd.read_csv('Gephi_stats/Gephi AGS Stats.csv')             \n",
    "caba_nd=pd.read_csv('Gephi_stats/Gephi CABA Stats.csv')           #capital\n",
    "cdmx_nd=pd.read_csv('Gephi_stats/Gephi CDMX Stats.csv')           #capital\n",
    "gdl_nd=pd.read_csv('Gephi_stats/Gephi GDL Stats.csv') \n",
    "hgo_nd=pd.read_csv('Gephi_stats/Gephi Hidalgo Stats.csv')\n",
    "mad_nd=pd.read_csv('Gephi_stats/Gephi Madrid Stats.csv')          #capital\n",
    "mtv_nd=pd.read_csv('Gephi_stats/Gephi Montevideo Stats.csv')      #capital\n",
    "oax_nd=pd.read_csv('Gephi_stats/Gephi Oaxaca Stats.csv')\n",
    "sao_nd=pd.read_csv('Gephi_stats/Gephi Sao Paulo Stats.csv')       #capital\n",
    "scl_nd=pd.read_csv('Gephi_stats/Gephi SCL Stats.csv')             #capital\n",
    "\n",
    "\n",
    "#-------------------------------------------------------\n",
    "# Archivo CSV describiendo las aristas de cada red\n",
    "#-------------------------------------------------------\n",
    "ags_ed=pd.read_csv('Gephi_edges/Gephi AGS Edges.csv')\n",
    "caba_ed=pd.read_csv('Gephi_edges/Gephi CABA Edges.csv')           #capital\n",
    "cdmx_ed=pd.read_csv('Gephi_edges/Gephi CDMX Edges.csv')           #capital\n",
    "gdl_ed=pd.read_csv('Gephi_edges/Gephi GDL Edges.csv')\n",
    "hgo_ed=pd.read_csv('Gephi_edges/Gephi Hidalgo Edges.csv')\n",
    "mad_ed=pd.read_csv('Gephi_edges/Gephi Madrid Edges.csv')          #capital\n",
    "mtv_ed=pd.read_csv('Gephi_edges/Gephi Montevideo Edges.csv')      #capital\n",
    "oax_ed=pd.read_csv('Gephi_edges/Gephi Oaxaca Edges.csv')\n",
    "sao_ed=pd.read_csv('Gephi_edges/Gephi Sao Paulo Edges.csv')       #capital\n",
    "scl_ed=pd.read_csv('Gephi_edges/Gephi SCL Edges.csv')             #capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def armar_grafo(nodes,edges,rol_str,weight_str):\n",
    "    '''\n",
    "    Función con la cual, a partir de una lista de nodos y conexiones, forma un grafo con NetworkX\n",
    "    \n",
    "    In:\n",
    "    - nodes       lista de nodos\n",
    "    - edges       lista de aristas\n",
    "    - rol_str     un nombre para el parámetro que describe el rol de un actor\n",
    "    - weight_str  un nombre para el parámetro que describe el peso de las aristas\n",
    "    \n",
    "    Out\n",
    "    Un objeto NetworkX llamado G.\n",
    "    '''\n",
    "    \n",
    "    #crea un grafo dirigido a partirde la lista edges\n",
    "    G=nx.from_pandas_edgelist(edges,'Source','Target',edge_attr=[\"Weight\"],create_using=nx.DiGraph())\n",
    "    \n",
    "    #rol es un diccionario que manda cada id de un nodo a el atributo correspondiente a rol\n",
    "    rol = {nid: nodes[nodes['Id']==nid][rol_str].values[0] for nid in nodes['Id']}\n",
    "    nx.set_node_attributes(G,rol,'rol')\n",
    "    \n",
    "    #weight es un diccionario que manda cada id de un nodo a el atributo correspondiente al peso de nodo\n",
    "    weight = {nid: nodes[nodes['Id']==nid][weight_str].values[0] for nid in nodes['Id']}\n",
    "    nx.set_node_attributes(G,weight,'weight')\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------\n",
    "#Armar grafos a partir de cada uno de los CSV que descargamos\n",
    "#--------------------------------------------------------------\n",
    "\n",
    "ags_G=armar_grafo(ags_nd,ags_ed,'role','weight')\n",
    "caba_G=armar_grafo(caba_nd,caba_ed,'type','weight')\n",
    "cdmx_G=armar_grafo(cdmx_nd,cdmx_ed,'rol estimado','weight')\n",
    "gdl_G=armar_grafo(gdl_nd,gdl_ed,'type','weight')\n",
    "hgo_G=armar_grafo(hgo_nd,hgo_ed,'type','weight')\n",
    "mad_G=armar_grafo(mad_nd,mad_ed,'rol estimado','weight')\n",
    "mtv_G=armar_grafo(mtv_nd,mtv_ed,'rol estimado','node size')\n",
    "oax_G=armar_grafo(oax_nd,oax_ed,'rol','weight')\n",
    "sao_G=armar_grafo(sao_nd,sao_ed,'rol estimado','weight')\n",
    "scl_G=armar_grafo(scl_nd,scl_ed,'type','weight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# Attribute Mixing Matrices - Original graphs\n",
    "#-------------------------------------------------\n",
    "#       Obtenemos una matriz cuya entrada (i,j) es la fracción de aristas \n",
    "#       que van de un eje con rol i a un eje con rol j, en la red original\n",
    "\n",
    "\n",
    "#cada rol lo asignamos a un índice del 1 al 5\n",
    "rol_map_esp={'Generador de conocimiento':0,'Habilitador':1,'Promotor':2,\n",
    "             'Vinculador':3,'Articulador':4,'Comunidad':5}\n",
    "\n",
    "rol_map_ing={'Knowledge Generator':0,'Enabler':1,\n",
    "             'Promoter':2,'Linker':3,'Articulator':4,'Community':5}\n",
    "\n",
    "capitals = {'CABA': caba_G, 'CDMX':cdmx_G, 'Santiago de Chile': scl_G, \n",
    "            'Montevideo': mtv_G, 'Madrid': mad_G, 'Sao Paulo': sao_G}\n",
    "\n",
    "#aquí vamos a almacenar las matrices\n",
    "attr_mix_matrices = capitals \n",
    "\n",
    "#conseguimos cada una de las attribute mixing matrices\n",
    "for capital_str in capitals.keys():\n",
    "    if capital_str=='Montevideo':\n",
    "        print('Matriz de artibuto Rol para la ciudad de '+capital_str)\n",
    "        print('')\n",
    "        M=nx.attribute_mixing_matrix(capitals[capital_str],'rol',mapping=rol_map_ing)\n",
    "        print(M)\n",
    "        print('')\n",
    "        print('')\n",
    "    else:\n",
    "        print('Matriz de artibuto Rol para la ciudad de '+capital_str)\n",
    "        print('')\n",
    "        M=nx.attribute_mixing_matrix(capitals[capital_str],'rol',mapping=rol_map_esp)\n",
    "        print(M)\n",
    "        print('')\n",
    "        print('')\n",
    "    attr_mix_matrices[capital_str] = M\n",
    "\n",
    "            \n",
    "# Creamos la instancia de una figura, con subplots\n",
    "fig = plt.figure(figsize = (20,20)) # ancho x alto\n",
    "ax1 = fig.add_subplot(3, 3, 1) # row, column, position\n",
    "ax1.set_title('Matriz de artibuto Rol para la ciudad de CABA')\n",
    "\n",
    "ax2 = fig.add_subplot(3, 3, 2)\n",
    "ax2.set_title('Matriz de artibuto Rol para la ciudad de CDMX')\n",
    "\n",
    "ax3 = fig.add_subplot(3, 3, 3)\n",
    "ax3.set_title('Matriz de artibuto Rol para la ciudad de Santiago de Chile')\n",
    "\n",
    "ax4 = fig.add_subplot(3, 3, 4)\n",
    "ax4.set_title('Matriz de artibuto Rol para la ciudad de Montevideo')\n",
    "\n",
    "ax5 = fig.add_subplot(3, 3, 5)\n",
    "ax5.set_title('Matriz de artibuto Rol para la ciudad de Madrid')\n",
    "\n",
    "ax6 = fig.add_subplot(3, 3, 6)\n",
    "ax6.set_title('Matriz de artibuto Rol para la ciudad de Sao Paulo')\n",
    "\n",
    "\n",
    "# We use ax parameter to tell seaborn which subplot to use for this plot\n",
    "sns.heatmap(data=attr_mix_matrices['CABA'], ax=ax1, square=True, cbar_kws={'shrink': .3}, annot=True, annot_kws={'fontsize': 12}, vmin=0.0, vmax=0.6)\n",
    "sns.heatmap(data=attr_mix_matrices['CDMX'], ax=ax2, square=True, cbar_kws={'shrink': .3}, annot=True, annot_kws={'fontsize': 12}, vmin=0.0, vmax=0.6)\n",
    "sns.heatmap(data=attr_mix_matrices['Santiago de Chile'], ax=ax3, square=True, cbar_kws={'shrink': .3}, annot=True, annot_kws={'fontsize': 12}, vmin=0.0, vmax=0.6)\n",
    "sns.heatmap(data=attr_mix_matrices['Montevideo'], ax=ax4, square=True, cbar_kws={'shrink': .3}, annot=True, annot_kws={'fontsize': 12}, vmin=0.0, vmax=0.6)\n",
    "sns.heatmap(data=attr_mix_matrices['Madrid'], ax=ax5, square=True, cbar_kws={'shrink': .3}, annot=True, annot_kws={'fontsize': 12}, vmin=0.0, vmax=0.6)      \n",
    "sns.heatmap(data=attr_mix_matrices['Sao Paulo'], ax=ax6, square=True, cbar_kws={'shrink': .3}, annot=True, annot_kws={'fontsize': 12}, vmin=0.0, vmax=0.6)      \n",
    "\n",
    "\n",
    "fig   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------\n",
    "# Subgrafos Nucleares \n",
    "#----------------------\n",
    "#    Obtenemos los -core- nodes (nodos núcleo) de cada una de las redes capitales, \n",
    "#    donde el grado total de un nodo núcleo es al menos 3\n",
    "\n",
    "caba_core=nx.k_core(caba_G,k=3)\n",
    "cdmx_core=nx.k_core(cdmx_G,k=3)\n",
    "scl_core=nx.k_core(scl_G,k=3)\n",
    "mad_core=nx.k_core(mad_G,k=3)\n",
    "mtv_core=nx.k_core(mtv_G,k=3)\n",
    "sao_core=nx.k_core(sao_G,k=3)\n",
    "\n",
    "#----------------------\n",
    "# GraphMLs Nucleares\n",
    "#----------------------\n",
    "#    Ahora, convertimos cada uno de los subgrafos nucleares que obtuvimos\n",
    "#    en un archivo .graphml\n",
    "\n",
    "nx.write_graphml(caba_core,'Gephi_core/CABA core graph.graphml')\n",
    "nx.write_graphml(cdmx_core,'Gephi_core/CDMX core graph.graphml')\n",
    "nx.write_graphml(scl_core,'Gephi_core/Scl core graph.graphml')\n",
    "nx.write_graphml(mad_core,'Gephi_core/Mad core graph.graphml')\n",
    "nx.write_graphml(mtv_core,'Gephi_core/Mtv core graph.graphml')\n",
    "nx.write_graphml(sao_core,'Gephi_core/Sao core graph.graphml')\n",
    "\n",
    "#----------------------\n",
    "# Subgrafos Corteza \n",
    "#----------------------\n",
    "#    Obtenemos los -crust- nodes (nodos corteza) de cada una de las redes capitales, \n",
    "#    donde el grado total de un nodo corteza es menor a 3\n",
    "\n",
    "caba_crust=nx.k_crust(caba_G,k=3)\n",
    "cdmx_crust=nx.k_crust(cdmx_G,k=3)\n",
    "scl_crust=nx.k_crust(scl_G,k=3)\n",
    "mad_crust=nx.k_crust(mad_G,k=3)\n",
    "mtv_crust=nx.k_crust(mtv_G,k=3)\n",
    "sao_crust=nx.k_crust(sao_G,k=3)\n",
    "\n",
    "#----------------------\n",
    "# GraphMLs Corteza  \n",
    "#----------------------\n",
    "#    Ahora, convertimos cada uno de los subgrafos corteza que obtuvimos\n",
    "#    en un archivo .graphml\n",
    "\n",
    "nx.write_graphml(caba_crust,'Gephi_crust/CABA crust graph.graphml')\n",
    "nx.write_graphml(cdmx_crust,'Gephi_crust/CDMX crust graph.graphml')\n",
    "nx.write_graphml(scl_crust,'Gephi_crust/Scl crust graph.graphml')\n",
    "nx.write_graphml(mad_crust,'Gephi_crust/Mad crust graph.graphml')\n",
    "nx.write_graphml(mtv_crust,'Gephi_crust/Mtv crust graph.graphml')\n",
    "nx.write_graphml(sao_crust,'Gephi_crust/Sao crust graph.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------\n",
    "#  Attribute Mixing Matrices - Core graphs\n",
    "#------------------------------------------------\n",
    "#     Para cada uno de los grafos corteza que conseguimos, \n",
    "#     Obtenemos una matriz cuya entrada (i,j) es la fracción \n",
    "#     de aristas que van de un eje con rol i a un eje con rol j\n",
    "\n",
    "\n",
    "rol_map_esp={'Generador de conocimiento':0,'Habilitador':1,'Promotor':2,\n",
    "             'Vinculador':3,'Articulador':4,'Comunidad':5}\n",
    "\n",
    "rol_map_ing={'Knowledge Generator':0,'Enabler':1,'Promoter':2,\n",
    "             'Linker':3,'Articulator':4,'Community':5}\n",
    "\n",
    "capitals_cores={'CABA': caba_core, 'CDMX':cdmx_core, 'Santiago de Chile': scl_core, \n",
    "                'Montevideo': mtv_core, 'Madrid': mad_core, 'Sao Paulo': sao_core}\n",
    "attr_mix_matrices = capitals_cores \n",
    "        \n",
    "\n",
    "\n",
    "for capital_str in capitals.keys():\n",
    "    if capital_str=='Montevideo':\n",
    "        print('Matriz de artibuto Rol para el núcleo (k=3) de '+capital_str)\n",
    "        print('')\n",
    "        M=nx.attribute_mixing_matrix(capitals_cores[capital_str],'rol',mapping=rol_map_ing)\n",
    "        print(M)\n",
    "        print('')\n",
    "        print('')\n",
    "    else:\n",
    "        print('Matriz de artibuto Rol para el núcleo (k=3) de '+capital_str)\n",
    "        print('')\n",
    "        M=nx.attribute_mixing_matrix(capitals_cores[capital_str],'rol',mapping=rol_map_esp)\n",
    "        print(M)\n",
    "        print('')\n",
    "        print('')\n",
    "    attr_mix_matrices[capital_str] = M\n",
    "\n",
    "        \n",
    "        \n",
    "# Creamos la instancia de una figura, con subplots\n",
    "fig = plt.figure(figsize = (20,20)) # width x height\n",
    "ax1 = fig.add_subplot(3, 3, 1) # row, column, position\n",
    "ax1.set_title('Matriz de artibuto Rol para el núcleo (k=3) de CABA')\n",
    "\n",
    "ax2 = fig.add_subplot(3, 3, 2)\n",
    "ax2.set_title('Matriz de artibuto Rol para el núcleo (k=3) de CDMX')\n",
    "\n",
    "ax3 = fig.add_subplot(3, 3, 3)\n",
    "ax3.set_title('Matriz de artibuto Rol para el núcleo (k=3) de Santiago de Chile')\n",
    "\n",
    "ax4 = fig.add_subplot(3, 3, 4)\n",
    "ax4.set_title('Matriz de artibuto Rol para el núcleo (k=3) de Montevideo')\n",
    "\n",
    "ax5 = fig.add_subplot(3, 3, 5)\n",
    "ax5.set_title('Matriz de artibuto Rol para el núcleo (k=3) de Madrid')\n",
    "\n",
    "ax6 = fig.add_subplot(3, 3, 6)\n",
    "ax6.set_title('Matriz de artibuto Rol para el núcleo (k=3) de Sao Paulo')\n",
    "\n",
    "\n",
    "# We use ax parameter to tell seaborn which subplot to use for this plot\n",
    "sns.heatmap(data=attr_mix_matrices['CABA'], ax=ax1, square=True, cbar_kws={'shrink': .3}, annot=True, annot_kws={'fontsize': 12}, vmin=0.0, vmax=0.6)\n",
    "sns.heatmap(data=attr_mix_matrices['CDMX'], ax=ax2, square=True, cbar_kws={'shrink': .3}, annot=True, annot_kws={'fontsize': 12}, vmin=0.0, vmax=0.6)\n",
    "sns.heatmap(data=attr_mix_matrices['Santiago de Chile'], ax=ax3, square=True, cbar_kws={'shrink': .3}, annot=True, annot_kws={'fontsize': 12}, vmin=0.0, vmax=0.6)\n",
    "sns.heatmap(data=attr_mix_matrices['Montevideo'], ax=ax4, square=True, cbar_kws={'shrink': .3}, annot=True, annot_kws={'fontsize': 12}, vmin=0.0, vmax=0.6)\n",
    "sns.heatmap(data=attr_mix_matrices['Madrid'], ax=ax5, square=True, cbar_kws={'shrink': .3}, annot=True, annot_kws={'fontsize': 12}, vmin=0.0, vmax=0.6)      \n",
    "sns.heatmap(data=attr_mix_matrices['Sao Paulo'], ax=ax6, square=True, cbar_kws={'shrink': .3}, annot=True, annot_kws={'fontsize': 12}, vmin=0.0, vmax=0.6)      \n",
    "\n",
    "\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------\n",
    "# Medidas Globales\n",
    "#----------------------------\n",
    "#     Ahora obtenemos, para cada una de las capitales, una serie \n",
    "#     pandas.core.series.Series, utilizando la función mean(), \n",
    "#     con la que obtenemos las siguientes centralidades \n",
    "#           - timeset\n",
    "#           - mentions    \n",
    "#           - avg strength                    promedio de la fuerza de los nodos\n",
    "#           - weight                          promedio del peso de los nodos\n",
    "#           - indegree                        promedio del grado hacia los nodos\n",
    "#           - outdegree                       promedio del grado fuera de los nodos\n",
    "#           - Degree                          promedio de in+out\n",
    "#           - weighted indegree               promedio del grado hacia los nodos con pesos de acuerdo a intensidades\n",
    "#           - weighted outdegree             promedio del grado fuera de los nodos con pesos de acuerdo a intensidades\n",
    "#           - Weighted Degree                promedio in+out de acuerdo a intensidades\n",
    "#           - Eccentricity                   promedio de eccentricidad \n",
    "#           - closnesscentrality              \n",
    "#           - harmonicclosnesscentrality      \n",
    "#           - betweenesscentrality          \n",
    "#           - modularity_class                \n",
    "#           - Authority                       promedio de autoridad de los nodos de acuerdo al aloritmo HITS\n",
    "#           - Hub                             promedio de Hubs de los nodos de acuerdo al aloritmo HITS\n",
    "#           - componentnumber                 número de componentes\n",
    "#           - strongcompnum                  número de componentes fuertemente conectados\n",
    "#           - clustering                     promedio de coeficiente de clustering\n",
    "#           - triangles                      \n",
    "#           - eigencentrality                 promedio de centralidades eigenvectores\n",
    "\n",
    "capitals_stats={'CABA': caba_nd, 'CDMX':cdmx_nd, 'Santiago de Chile': scl_nd, \n",
    "                'Montevideo': mtv_nd, 'Madrid': mad_nd, 'Sao Paulo': sao_nd}\n",
    "\n",
    "\n",
    "averages={city: stats.mean() for city,stats in capitals_stats.items()}\n",
    "#averages['Sao Paulo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejemplo: Aquí vamos a conseguir el CSV para el núcleo de CDMX, en donde conocemos algunas de sus métricas\n",
    "\n",
    "city='CDMX'\n",
    "avg=averages[city]\n",
    "datafr=avg.copy().to_frame().T\n",
    "datafr.drop(['timeset'], axis=1, inplace=True)\n",
    "if 'type' in datafr.columns:\n",
    "    datafr.drop(['type'], axis=1, inplace=True)\n",
    "\n",
    "all_columns = datafr.columns.values.tolist()\n",
    "print(all_columns)\n",
    "print('')\n",
    "print('')\n",
    "print('')\n",
    "datafr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries={'Montevideo':'Uruguay', 'CABA':'Argentina', 'CDMX':'México',\n",
    "          'Madrid':'España', 'Sao Paulo': 'Brasil', 'Santiago de Chile': 'Chile'}\n",
    "\n",
    "\n",
    "list_concat=[]\n",
    "for city,avg in averages.items():\n",
    "    datafr=avg.copy().to_frame().T\n",
    "    \n",
    "    #borramos del set de columnas a aquellos atributos que no son significantes \n",
    "    datafr.drop(['timeset', 'componentnumber'], axis=1, inplace=True)\n",
    "    if 'type' in datafr.columns:\n",
    "        datafr.drop(['type'], axis=1, inplace=True)\n",
    "    \n",
    "    #añadimos datafr a la lista de dataframes que vamos a concatenar\n",
    "    list_concat.append(datafr)\n",
    "    \n",
    "    #añadimos el atributo que corresponde a el nombre de ciudad y país\n",
    "    datafr.insert(0, 'País', [countries[city]], True) \n",
    "    datafr.insert(0, 'Ciudad', [city], True) \n",
    "\n",
    "df_concat=pd.concat(list_concat, ignore_index=True)\n",
    "df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quitar promedios que en la pagina no mencionan que describen una propiedad global del grafo\n",
    "\n",
    "df_concat=df_concat.drop('closnesscentrality',axis=1)\n",
    "df_concat=df_concat.drop('harmonicclosnesscentrality',axis=1)\n",
    "df_concat=df_concat.drop('modularity_class',axis=1)\n",
    "df_concat=df_concat.drop('triangles',axis=1)\n",
    "df_concat=df_concat.drop('eigencentrality',axis=1)\n",
    "df_concat=df_concat.drop('pageranks',axis=1)\n",
    "df_concat=df_concat.drop('ego',axis=1)\n",
    "df_concat=df_concat.drop('betweenesscentrality',axis=1)\n",
    "df_concat=df_concat.drop('Hub',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat['weight'][3]=df_concat['node size'][3]\n",
    "df_concat=df_concat.drop('node size',axis=1)\n",
    "df_concat=df_concat.drop('strongcompnum',axis=1)\n",
    "df_concat=df_concat.drop('Authority',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Global characteristics we may compute:\n",
    "\n",
    "*Diameter  \n",
    "*Radius\n",
    "**Average path length\n",
    "**Transitivity\n",
    "*Global Efficiency\n",
    "***Modularity\n",
    "*Assortativity Coefficient\n",
    "Small Worldness\n",
    "\n",
    "\n",
    "*   = computed already, not defined for direted graph\n",
    "**  = computed for undirected graph, also defined for directed graph\n",
    "*** = computed manually\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------\n",
    "# Diameter\n",
    "#--------------\n",
    "capitals={'CABA': caba_G, 'CDMX':cdmx_G, 'Santiago de Chile': scl_G, \n",
    "          'Montevideo': mtv_G, 'Madrid': mad_G, 'Sao Paulo': sao_G}\n",
    "\n",
    "\n",
    "print('-------------------------------------------')\n",
    "print('Diámetro en Cores dirigidos')\n",
    "print('-------------------------------------------')\n",
    "for city, graph in capitals.items():\n",
    "    try:\n",
    "        print(city+str(': ')+str(nx.diameter(graph)))\n",
    "        print('')\n",
    "    except:\n",
    "        print(city+str(': ')+str(float('inf')))\n",
    "        print('')\n",
    "\n",
    "print('-------------------------------------------')\n",
    "print('Diámetro en Cores no dirigidos')\n",
    "print('-------------------------------------------')\n",
    "for city, graph in capitals.items():\n",
    "    try:\n",
    "        print(city+str(': ')+str(nx.diameter(nx.to_undirected(graph))))\n",
    "        print('')\n",
    "    except:\n",
    "        print(city+str(': ')+str(float('inf')))\n",
    "        print('')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------\n",
    "# Radius\n",
    "#--------------\n",
    "\n",
    "print('-------------------------------------------')\n",
    "print('Radio en Cores dirigidos')\n",
    "print('-------------------------------------------')\n",
    "for city, graph in capitals.items():\n",
    "    try:\n",
    "        print(city+str(': ')+str(nx.radius(graph)))\n",
    "        print('')\n",
    "    except:\n",
    "        print(city+str(': ')+str(float('inf')))\n",
    "        print('')\n",
    "\n",
    "print('-------------------------------------------')\n",
    "print('Radio en Cores no dirigidos')\n",
    "print('-------------------------------------------')\n",
    "for city, graph in capitals.items():\n",
    "    try:\n",
    "        print(city+str(': ')+str(nx.radius(nx.to_undirected(graph))))\n",
    "        print('')\n",
    "    except:\n",
    "        print(city+str(': ')+str(float('inf')))\n",
    "        print('') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------\n",
    "# Camino más corto promedio\n",
    "#---------------------------------\n",
    "\n",
    "# parece que aunque las graficas dirigidas no sean conexas, el camino \n",
    "# más corto promedio si se está pudiendo computar, y nos da un resultado.. menor?\n",
    "\n",
    "print('-------------------------------------------')\n",
    "print('Camino más corto promedio en Cores dirigidos')\n",
    "print('-------------------------------------------')\n",
    "for city, graph in capitals.items():\n",
    "    try:\n",
    "        print(city+str(': ')+str(nx.average_shortest_path_length(graph)))\n",
    "        print('')\n",
    "    except:\n",
    "        print(city+str(': ')+str(float('inf')))\n",
    "        print('')\n",
    "\n",
    "print('-------------------------------------------')\n",
    "print('Camino más corto promedio en Cores no dirigidos')\n",
    "print('-------------------------------------------')\n",
    "for city, graph in capitals.items():\n",
    "    try:\n",
    "        print(city+str(': ')+str(nx.average_shortest_path_length(nx.to_undirected(graph))))\n",
    "        print('')\n",
    "    except:\n",
    "        print(city+str(': ')+str(float('inf')))\n",
    "        print('')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------\n",
    "# Transitividad\n",
    "#---------------------------------\n",
    "\n",
    "# parece que aunque las graficas dirigidas no sean conexas, el camino \n",
    "# más corto promedio si se está pudiendo computar, y nos da un resultado.. menor?\n",
    "print('----------------------------------')\n",
    "print('Transitividad en Cores dirigidos')\n",
    "print('----------------------------------')\n",
    "\n",
    "for city, graph in capitals.items():\n",
    "    try:\n",
    "        print(city+str(': ')+str(nx.transitivity(graph)))\n",
    "        print('')\n",
    "    except:\n",
    "        print(city+str(': ')+str(float('inf')))\n",
    "        print('')\n",
    "\n",
    "print('----------------------------------')\n",
    "print('Transitividad en Cores no dirigidos')\n",
    "print('----------------------------------')\n",
    "\n",
    "for city, graph in capitals.items():\n",
    "    try:\n",
    "        print(city+str(': ')+str(nx.transitivity(nx.to_undirected(graph))))\n",
    "        print('')\n",
    "    except:\n",
    "        print(city+str(': ')+str(float('inf')))\n",
    "        print('') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------\n",
    "# Eficiencia Global\n",
    "#-----------------------\n",
    "# Solo esta definida para las versiones no dirigidas de nuestras redes\n",
    "\n",
    "print('----------------------------------')\n",
    "print('Eficiencia Global en Cores dirigidos')\n",
    "print('----------------------------------')\n",
    "\n",
    "for city, graph in capitals.items():\n",
    "    try:\n",
    "        print(city+str(': ')+str(nx.global_efficiency(graph)))\n",
    "        print('')\n",
    "    except:\n",
    "        print(city+str(': ')+str(float('inf')))\n",
    "        print('')\n",
    "\n",
    "print('----------------------------------')\n",
    "print('Eficiencia Global en Cores no dirigidos')\n",
    "print('----------------------------------')\n",
    "\n",
    "for city, graph in capitals.items():\n",
    "    try:\n",
    "        print(city+str(': ')+str(nx.global_efficiency(nx.to_undirected(graph))))\n",
    "        print('')\n",
    "    except:\n",
    "        print(city+str(': ')+str(float('inf')))\n",
    "        print('') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------\n",
    "# Modularidad\n",
    "#-------------------\n",
    "\n",
    "#modularidad Parece no estar definido en este esquema, pero podemos obtener esta medida de Gephi\n",
    "\n",
    "print('----------------------------------')\n",
    "print('Modularidad en Cores dirigidos')\n",
    "print('----------------------------------')\n",
    "\n",
    "for city, graph in capitals.items():\n",
    "    try:\n",
    "        print(city+str(': ')+str(nx.community.modularity(graph)))\n",
    "        print('')\n",
    "    except:\n",
    "        print(city+str(': ')+str(float('inf')))\n",
    "        print('')\n",
    "\n",
    "print('----------------------------------')\n",
    "print('Modularidad en Cores no dirigidos')\n",
    "print('----------------------------------')\n",
    "\n",
    "for city, graph in capitals.items():\n",
    "    try:\n",
    "        print(city+str(': ')+str(nx.community.modularity(nx.to_undirected(graph))))\n",
    "        print('')\n",
    "    except:\n",
    "        print(city+str(': ')+str(float('inf')))\n",
    "        print('') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------\n",
    "# Assortativity coefficient\n",
    "#---------------------------\n",
    "\n",
    "print('----------------------------------')\n",
    "print('Coef. Asortatividad en Cores dirigidos')\n",
    "print('----------------------------------')\n",
    "\n",
    "for city, graph in capitals.items():\n",
    "    try:\n",
    "        print(city+str(': ')+str(nx.degree_assortativity_coefficient(graph)))\n",
    "        print('')\n",
    "    except:\n",
    "        print(city+str(': ')+str(float('inf')))\n",
    "        print('')\n",
    "\n",
    "print('----------------------------------')\n",
    "print('Coef. Asortatividad en Cores no dirigidos')\n",
    "print('----------------------------------')\n",
    "\n",
    "for city, graph in capitals.items():\n",
    "    try:\n",
    "        print(city+str(': ')+str(nx.degree_assortativity_coefficient(nx.to_undirected(graph))))\n",
    "        print('')\n",
    "    except:\n",
    "        print(city+str(': ')+str(float('inf')))\n",
    "        print('') \n",
    "\n",
    "d1 = oax_G.to_undirected().degree('IODEMC')\n",
    "d2 = scl_G.to_undirected().degree('CORFO')\n",
    "print(d1, d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------\n",
    "# Small Worldness\n",
    "#------------------\n",
    "\n",
    "#networkx.algorithms.smallworld.random_reference\n",
    "print('----------------------------------')\n",
    "print('Small Worldness en Cores dirigidos')\n",
    "print('----------------------------------')\n",
    "\n",
    "for city, graph in capitals.items():\n",
    "    try:\n",
    "        print(city+str(': ')+str(nx.algorithms.smallworld.sigma(graph,niter=1,nrand=2)))\n",
    "        print('')\n",
    "    except:\n",
    "        print(city+str(': ')+str(float('inf')))\n",
    "        print('')\n",
    "\n",
    "print('----------------------------------')\n",
    "print('Small Worldness en Cores no dirigidos')\n",
    "print('----------------------------------')\n",
    "\n",
    "for city, graph in capitals.items():\n",
    "    try:\n",
    "        print(city+str(': ')+str(nx.algorithms.smallworld.sigma(graph.to_undirected(),niter=1,nrand=2)))\n",
    "        print('')\n",
    "    except:\n",
    "        print(city+str(': ')+str(float('inf')))\n",
    "        print('') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------\n",
    "#  Modularidades\n",
    "#-----------------\n",
    "#     Estas medidas fueron tomadas no por medio de este programa, sino manualmente desde el programa Gephi\n",
    "#     Metodología:\n",
    "#        Tomamos tres medidas en cada red, con cada una de las resoluciones 0.8, 1.0, 1.2, con o sin ejes\n",
    "#\n",
    "#        Network       (CABA, CDMX, MAD, MTV SAO, SCL)\n",
    "#        Resolution    (0.8, 1.0, 1.2)  \n",
    "#        Edge weights  (Y/N)\n",
    "#        \n",
    "#        Y luego de ello tomamos la mediana de las tres medidas de resolución, a modo de tomar en cuenta variabilidad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_concat['Modularidad con pesos (0.8)'] = [0.402, 0.484, 0.530, 0.472, 0.611, 0.686]\n",
    "df_concat['Modularidad con pesos (1.0)'] = [0.524, 0.628, 0.530, 0.611, 0.610, 0.682]\n",
    "df_concat['Modularidad con pesos (1.2)'] = [0.654, 0.776, 0.524, 0.756, 0.612, 0.673]\n",
    "\n",
    "\n",
    "df_concat['Modularidad sin pesos (0.8)'] = [0.497, 0.602, 0.496, 0.353, 0.568, 0.673]\n",
    "df_concat['Modularidad sin pesos (1.0)'] = [0.493, 0.604, 0.500, 0.348, 0.572, 0.664]\n",
    "df_concat['Modularidad sin pesos (1.2)'] = [0.499, 0.601, 0.505, 0.337, 0.563, 0.661]\n",
    "\n",
    "df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals = {'CABA': caba_G, 'CDMX':cdmx_G, 'Santiago de Chile': scl_G, 'Montevideo': mtv_G, 'Madrid': mad_G, 'Sao Paulo': sao_G}\n",
    "\n",
    "#las columnas que vamos a aregar al DataFrame\n",
    "diameter = [0]*len(capitals)\n",
    "radius = [0]*len(capitals)\n",
    "avg_shortest_path_length = [0]*len(capitals)\n",
    "transitivity = [0]*len(capitals)\n",
    "global_efficiency = [0]*len(capitals)\n",
    "modularity = [0]*len(capitals)\n",
    "\n",
    "small_worldness = [0]*len(capitals)\n",
    "\n",
    "\n",
    "for city, graph in capitals.items():\n",
    "    city_index=df_concat.index[df_concat['Ciudad']==city].tolist()[0]\n",
    "    \n",
    "    #conseguimos cada parámetro para esta ciudad\n",
    "    undirected=nx.to_undirected(graph)\n",
    "    diameter[city_index] =                     nx.diameter(undirected)\n",
    "    radius[city_index] =                       nx.radius(undirected)\n",
    "    avg_shortest_path_length[city_index] =     nx.average_shortest_path_length(undirected)\n",
    "    transitivity[city_index] =                 nx.transitivity(undirected)\n",
    "    global_efficiency[city_index] =            nx.global_efficiency(undirected)\n",
    "    small_worldness[city_index] =              nx.nx.algorithms.smallworld.sigma(graph.to_undirected(),niter=1,nrand=5)\n",
    "\n",
    "    \n",
    "    #espacio para modularidad\n",
    "    \n",
    "    #deg_assortativity_coefficient[city_index]= nx.degree_assortativity_coefficient(undirected)\n",
    "    \n",
    "df_concat['Diámetro'] = diameter\n",
    "df_concat['Radio'] = radius\n",
    "df_concat['Camino más corto promedio'] = avg_shortest_path_length\n",
    "df_concat['Transitividad'] = transitivity\n",
    "df_concat['Eficiencia Global'] = global_efficiency\n",
    "#df_concat['Coeficiente de Asortatividad de Grado'] = deg_assortativity_coefficient\n",
    "df_concat['Small Worldness']= small_worldness\n",
    "df_concat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cambiar los nombres de los atributos del DataFrame\n",
    "#algunos nombres de columnas no fueron modificados para facilitar legibilidad\n",
    "df_concat.rename(columns={'mentions': 'Menciones', 'Eccentricity': 'Eccentricidad', 'clustering':'Clustering'}, inplace=True)\n",
    "df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat.to_csv('Tidy_DataFrame.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorando el caso de Oaxaca y Santiago de Chile\n",
    "\n",
    "Una de las observaciones que hizo el equipo de GED fue acerca de las redes de Oaxaca y Santiago de Chile. En Oaxaca, el nodo más prominente es IODEMC, mientras que en SCL el nodo más prominente es CORFO. Ambos nodos presentan mayor número de menciones, mayor número de flechas dirigidas hacia dichos nodos, y mayor grado. Por ende, estos nodos también resultan prominentes en casi todas las métricas nodales. \n",
    "\n",
    "Sin embargo, al remover IODEMC de Oaxaca, la red se desestabiliza:\n",
    "- aloha \n",
    "\n",
    "Por el otro lado, al remover CORFO de SCL, la red no logra desestabilizarse tanto como la red previa. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean \n",
    "from statistics import stdev "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------\n",
    "# Triangles\n",
    "#------------------------\n",
    "\n",
    "#obtaining IODEMC's triangle number in OAX's network\n",
    "print(nx.triangles(oax_G.to_undirected(), 'IODEMC'))\n",
    "\n",
    "#obtaining CORFO's triangle number in SCL's network\n",
    "print(nx.triangles(scl_G.to_undirected(), 'CORFO'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------\n",
    "# Path length\n",
    "#------------------------\n",
    "#average shortest path length from any other node to our nodes of interest.\n",
    "\n",
    "#obtaining IODEMC's path length in OAX's network\n",
    "print( mean(nx.shortest_path_length(oax_G, 'IODEMC').values()) )\n",
    "\n",
    "#obtaining CORFO's path length in SCL's network\n",
    "print( mean(nx.shortest_path_length(scl_G, 'CORFO').values()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------\n",
    "# Global efficiency\n",
    "#------------------------\n",
    "#average shortest path length from any other node to our nodes of interest.\n",
    "\n",
    "#obtaining IODEMC's global efficiency in OAX's network\n",
    "shortest_path_lengths_oax = nx.shortest_path_length(oax_G, 'IODEMC')\n",
    "del (shortest_path_lengths_oax['IODEMC'])\n",
    "print( mean(shortest_path_lengths_oax.values()) ) \n",
    "\n",
    "#obtaining CORFO's global efficiency in SCL's network\n",
    "shortest_path_lengths_scl = nx.shortest_path_length(scl_G, 'CORFO')\n",
    "del (shortest_path_lengths_scl['CORFO'])\n",
    "print( mean(shortest_path_lengths_scl.values()) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------\n",
    "# Local efficiency\n",
    "#------------------------\n",
    "#average shortest path length from any neighbor node to our nodes of interest.\n",
    "\n",
    "def nodes_connected(G, u, v):\n",
    "    return u in G.neighbors(v)\n",
    "\n",
    "\n",
    "#obtaining IODEMC's global efficiency in OAX's network\n",
    "shortest_paths_ngb_oax = {key:value for key,value in shortest_path_lengths_oax.items() if nodes_connected(oax_G, 'IODEMC', key)}\n",
    "print( mean([1/x for x in shortest_paths_ngb_oax.values()]) ) \n",
    "\n",
    "#obtaining CORFO's global efficiency in SCL's network\n",
    "shortest_paths_ngb_scl = {key:value for key,value in shortest_path_lengths_scl.items() if nodes_connected(scl_G, 'CORFO', key)}\n",
    "print( mean([1/x for x in shortest_paths_ngb_scl.values()]) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------\n",
    "# Within module z-score\n",
    "#------------------------\n",
    "#   Degree to which a node is connected to other nodes inside the same community\n",
    "#   Helper functions:\n",
    "\n",
    "def nodes_connected(G, u, v):\n",
    "    return u in G.neighbors(v)\n",
    "\n",
    "def deg_in_module(G, M, v):\n",
    "    #degree of node inside its community\n",
    "    k=0                                                      \n",
    "    for x in M:\n",
    "        if nodes_connected(G, v, x) or nodes_connected(G, x, v): k+=1\n",
    "    return k\n",
    "\n",
    "\n",
    "#    Node:       IODEMC\n",
    "#    Network:    oax_G\n",
    "#    Community:  Articulators\n",
    "\n",
    "articulators = [x for x,y in oax_G.nodes(data=True) if y['rol']=='Articulador']\n",
    "ki = deg_in_module(oax_G, articulators, 'IODEMC')                                 #deg in module of IODEMC\n",
    "in_mod_degrees = [deg_in_module(oax_G, articulators, v) for v in articulators ]\n",
    "ksi = mean(in_mod_degrees)                                                        #average degree \n",
    "sigmasi = stdev(in_mod_degrees)                                                   #standard deviation of degrees\n",
    "\n",
    "\n",
    "print('K_i = '+str(ki) )\n",
    "print('K_Si = '+str(ksi) )\n",
    "print('Sigma_Si = '+str(sigmasi) )\n",
    "print('IODEMCs within module z-score is: '+ str((ki-ksi)/sigmasi) )\n",
    "print('-----------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "#    Node:       CORFO\n",
    "#    Network:    oax_G\n",
    "#    Community:  Articulators\n",
    "\n",
    "articulators = [x for x,y in scl_G.nodes(data=True) if y['rol']=='Articulador']\n",
    "ki = deg_in_module(scl_G, articulators, 'CORFO')                                 #deg in module of IODEMC\n",
    "in_mod_degrees = [deg_in_module(scl_G, articulators, v) for v in articulators ]\n",
    "ksi = mean(in_mod_degrees)                                                        #average degree \n",
    "sigmasi = stdev(in_mod_degrees)                                                   #standard deviation of degrees\n",
    "\n",
    "\n",
    "print('K_i = '+str(ki) )\n",
    "print('K_Si = '+str(ksi) )\n",
    "print('Sigma_Si = '+str(sigmasi) )\n",
    "print('CORFOs within module z-score is: '+ str((ki-ksi)/sigmasi) )\n",
    "print('-----------------------------------------------')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#----------------------------\n",
    "# Participation coefficient\n",
    "#----------------------------\n",
    "#   Relation between edges connecting node with other communities, and total number of edges from node\n",
    "#   Helper functions:\n",
    "\n",
    "def nodes_connected(G, u, v):\n",
    "    return u in G.neighbors(v)\n",
    "\n",
    "def deg_in_module(G, M, v):\n",
    "    #degree of node inside its community\n",
    "    k=0                                                      \n",
    "    for x in M:\n",
    "        if nodes_connected(G, v, x) or nodes_connected(G, x, v): k+=1\n",
    "    return k\n",
    "\n",
    "\n",
    "#    Node:       IODEMC\n",
    "#    Network:    oax_G\n",
    "#    Community:  Articulators\n",
    "\n",
    "roles=['Articulador', 'Habilitador', 'Generador de conocimiento', 'Vinculador', 'Comunidad', 'Promotor']\n",
    "communities = {rol: [x for x,y in oax_G.nodes(data=True) if y['rol']==rol] for rol in roles}\n",
    "d = oax_G.degree('IODEMC')\n",
    "print(d)\n",
    "p=1\n",
    "for rol in roles:\n",
    "    p -= (deg_in_module(oax_G, communities[rol], 'IODEMC'))**2 / d**2    #we want to check degree of 'IODEMC' \n",
    "\n",
    "print('IODEMCs participation coefficient is: '+ str(p) )\n",
    "print('-----------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "#    Node:       CORFO\n",
    "#    Network:    oax_G\n",
    "#    Community:  Articulators\n",
    "\n",
    "roles=['Articulador', 'Habilitador', 'Generador de conocimiento', 'Vinculador', 'Comunidad', 'Promotor']\n",
    "communities = {rol: [x for x,y in scl_G.nodes(data=True) if y['rol']==rol] for rol in roles}\n",
    "d = scl_G.degree('CORFO')\n",
    "print(d)\n",
    "p=1\n",
    "for rol in roles:\n",
    "    p -= (deg_in_module(scl_G, communities[rol], 'CORFO'))**2 / d**2    #we want to check degree of 'IODEMC' \n",
    "\n",
    "print('CORFOs participation coefficient is: '+ str(p) )\n",
    "print('-----------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
